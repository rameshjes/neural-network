{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sympy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Backpropagation:\n",
    "\n",
    "    def __init__(self,\n",
    "                 _no_input_neuron,\n",
    "                 _hidden_config,\n",
    "                 _no_output_neuron,\n",
    "                 _input_data_set,\n",
    "                 _desired_output,\n",
    "                 _epoch):\n",
    "        self.epochs = _epoch\n",
    "\n",
    "        self.no_of_input_neurons = _no_input_neuron\n",
    "        self._hidden_config = _hidden_config\n",
    "        self.no_of_layers = len(_hidden_config)\n",
    "        self.no_of_hidden_neurons = _hidden_config[0]\n",
    "        self.no_of_output_neurons = _no_output_neuron\n",
    "\n",
    "        self.input_data_set = _input_data_set\n",
    "        self.desired_output = _desired_output\n",
    "        \n",
    "        self.eta = 0.03\n",
    "        \n",
    "    def generate_weight(self,is_random,_size):\n",
    "        return np.random.uniform(size=_size) if is_random else np.zeros(_size)\n",
    "        \n",
    "    def sigmoid (self,x): \n",
    "        return 1/(1 + np.exp(-x)) \n",
    "    \n",
    "    def derivative_(self,x): \n",
    "        return x * (1 - x)\n",
    "    \n",
    "    def local_field(self,x,w):\n",
    "        return np.dot(x,w)\n",
    "    \n",
    "    def error(self,y):\n",
    "        return self.desired_output - y\n",
    "    \n",
    "    def delta(self,sigma_tic,summed_error,flag = False):\n",
    "        return summed_error * sigma_tic        \n",
    "#         return np.dot(sigma_tic,summed_error) if flag else sigma_tic*summed_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bp = Backpropagation(2,\n",
    "                     [2],\n",
    "                     1,\n",
    "                     np.array([[0,0],[0,1],[1,0],[1,1]]),\n",
    "                     np.array([[0],[1],[1],[0]]),\n",
    "                     5000)\n",
    "Wh = np.random.uniform(size=(bp.no_of_input_neurons, bp._hidden_config[0]))\n",
    "Wz = np.random.uniform(size=(bp._hidden_config[0],bp.no_of_output_neurons))\n",
    "\n",
    "for i in range(bp.epochs):\n",
    "    H = bp.sigmoid(np.dot(bp.input_data_set, Wh))                  # hidden layer results\n",
    "    Z = bp.sigmoid(np.dot(H, Wz))                  # output layer results\n",
    "    E = bp.desired_output - Z\n",
    "#     print E\n",
    "    dZ = E * bp.derivative_(Z) \n",
    "    dH = dZ.dot(Wz.T) * bp.derivative_(H)             # delta H\n",
    "    Wz +=  0.1 * H.T.dot(dZ)                          # update output layer weights\n",
    "    Wh +=  0.1 * bp.input_data_set.T.dot(dH) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Exercise 3\n",
    "Investigate the use of back-propagation learning using a sigmoidal nonlinearity to achieve one-toone mappings, as described here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate data based on minimum and maximum value, n is no. of samples\n",
    "def get_data(minimum,maximum,n):\n",
    "    data = np.zeros((1,n))\n",
    "    for i in range(n):\n",
    "        data[0,i] = random.uniform(minimum, maximum)\n",
    "    return data\n",
    "    \n",
    "#Training phase\n",
    "#Wh and Wz are hidden and output layer weights\n",
    "def training(training_set,desired,Wh,Wz):\n",
    "    \n",
    "    epoch_e = []\n",
    "    squared_error = []\n",
    "    epoch_count = 0\n",
    "    while(True):\n",
    "        #Forward pass\n",
    "        vh = bp.local_field(training_set.T,Wh)\n",
    "        sigmoid = bp.sigmoid(vh)\n",
    "        vo = bp.local_field(sigmoid,Wz)\n",
    "        y = bp.sigmoid(vo)\n",
    "        e = desired.T - y\n",
    "        epoch_e.append(e)\n",
    "    \n",
    "        #Backward pass\n",
    "        dZ = e * bp.derivative_(y) \n",
    "        dH = dZ.dot(Wz.T) * bp.derivative_(sigmoid)\n",
    "        Wz +=  -eta * np.dot(sigmoid.T,dZ)\n",
    "        Wh +=  -eta * training_set.dot(dH)\n",
    "        \n",
    "        if epoch_count>0:\n",
    "            squared_error.append((epoch_e[epoch_count]**2-epoch_e[epoch_count-1]**2))\n",
    "            # If average squared error is less than 0.01, we stop adjustment\n",
    "            if  (np.average(squared_error[epoch_count-1])) < 0.01:\n",
    "                break\n",
    "\n",
    "        epoch_count +=1\n",
    "    print \"Training phase\"\n",
    "    print \"Number of epochs it took: \", epoch_count\n",
    "    return (Wh,Wz)\n",
    "\n",
    "# Test data using weight adjusted during training phase\n",
    "def testing(test_data,Wh,Wz):\n",
    "    print \"Testing\"\n",
    "    test_vh = bp.local_field(test_data.T,Wh)\n",
    "    test_sigmoid = bp.sigmoid(test_vh)\n",
    "    test_vo = bp.local_field(test_sigmoid,Wz)\n",
    "    output_mapping = bp.sigmoid(test_vo)\n",
    "    return output_mapping\n",
    "\n",
    "\n",
    "'''\n",
    "To compute accuracy: \n",
    "    i. We check how much test data is classified correctly, \n",
    "     based on weights adjusted during training phase\n",
    "'''\n",
    "def compute_accuracy(training_data,test_set,\n",
    "                     desired,expected,\n",
    "                     nHidden,nOutput):\n",
    "    \n",
    "    \n",
    "    Wh = np.random.rand(1, nHidden)\n",
    "    Wz = np.random.rand(nHidden,nOutput)\n",
    "\n",
    "    Wh,Wz = training(training_data,desired,Wh,Wz)\n",
    "    print \"Adjusted weights from training phase :\"\n",
    "    print \"hidden weights \", Wh\n",
    "    print \"Output weights \", Wz\n",
    "    #we use sample weights for testing\n",
    "    actual = testing(test_set,Wh,Wz)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generate Training data\n",
    "\n",
    "#get_data takens minimum, maximum, number_of_samples\n",
    "#f(x) = 1/x\n",
    "training_set1 = get_data(1,100,200)\n",
    "desired1 = np.asarray([1.0/x for x in training_set1])\n",
    "\n",
    "#f(x) = log_10(x)\n",
    "training_set2 = get_data(1,10,20)\n",
    "desired2 = np.asarray([np.log10(x) for x in training_set2])\n",
    "\n",
    "#f(x) = exp(-x)\n",
    "training_set3 = get_data(1,10,20)\n",
    "desired3 = np.asarray([np.exp(-x) for x in training_set3])\n",
    "\n",
    "#f(x) = sin(x)\n",
    "training_set4 = get_data(1,45,20)\n",
    "desired4 = np.asarray([np.sin(x) for x in training_set4])\n",
    "\n",
    "#Generate Test sets\n",
    "# we generate half numebr of samples for test as compare to training\n",
    "test_set1 = get_data(1,100,100)\n",
    "expected1 = np.asarray([1.0/x for x in test_set1])\n",
    "\n",
    "test_set2 = get_data(1,10,10)\n",
    "expected2 = np.asarray([np.log10(x) for x in test_set2])\n",
    "\n",
    "test_set3 = get_data(1,10,10)\n",
    "expected3 = np.asarray([np.exp(-x) for x in test_set3])\n",
    "\n",
    "test_set4 = get_data(1,45,10)\n",
    "expected4 = np.asarray([np.sin(np.radians(x)) for x in test_set4])\n",
    "\n",
    "\n",
    "nHLayers = 1 #hidden layers\n",
    "nOutput = 1 #hidden neurons\n",
    "eta = 0.3 #learning rate\n",
    "nHidden = [3] # number of hidden  neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training phase\n",
      "Number of epochs it took:  2\n",
      "Adjusted weights from training phase :\n",
      "hidden weights  [[  0.96595096   0.31428305  12.30265004]]\n",
      "Output weights  [[ 8.40216971]\n",
      " [ 8.14183673]\n",
      " [ 6.74096635]]\n",
      "Testing\n"
     ]
    }
   ],
   "source": [
    "#i) f(x) = 1/x\n",
    "for i in range(len(nHidden)):\n",
    "    compute_accuracy(training_set1,test_set1,desired1,expected1,nHidden[i],nOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training phase\n",
      "Number of epochs it took:  1\n",
      "Adjusted weights from training phase :\n",
      "hidden weights  [[ 0.96116125  0.85601473  0.54494088]]\n",
      "Output weights  [[ 0.98758086]\n",
      " [ 1.05909418]\n",
      " [ 0.89963497]]\n",
      "Testing\n"
     ]
    }
   ],
   "source": [
    "#i) f(x) = log(x)\n",
    "for i in range(len(nHidden)):\n",
    "    compute_accuracy(training_set2,test_set2,desired2,expected2,nHidden[i],nOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training phase\n",
      "Number of epochs it took:  4\n",
      "Adjusted weights from training phase :\n",
      "hidden weights  [[ 0.70079645  0.91027208  0.74596715]]\n",
      "Output weights  [[ 1.75172832]\n",
      " [ 1.15067955]\n",
      " [ 1.92467334]]\n",
      "Testing\n"
     ]
    }
   ],
   "source": [
    "#i) f(x) = exp(-x)\n",
    "for i in range(len(nHidden)):\n",
    "    compute_accuracy(training_set3,test_set3,desired3,expected3,nHidden[i],nOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training phase\n",
      "Number of epochs it took:  4\n",
      "Adjusted weights from training phase :\n",
      "hidden weights  [[ 0.88332294  0.59444611  0.62668124]]\n",
      "Output weights  [[ 1.17280332]\n",
      " [ 1.77834417]\n",
      " [ 1.63295171]]\n",
      "Testing\n"
     ]
    }
   ],
   "source": [
    "#i) f(x) = sin(x)\n",
    "for i in range(len(nHidden)):\n",
    "    compute_accuracy(training_set4,test_set4,desired4,expected4,nHidden[i],nOutput)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
