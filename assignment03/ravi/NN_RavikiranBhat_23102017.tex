\documentclass[12pt]{article}
\usepackage{geometry}
\usepackage{url}
\usepackage{graphicx}
\usepackage{float}
\usepackage{pdflscape}
\usepackage{amsmath}
\title{\Huge Neural Networks \\
[6mm]
Assignment 3\\}
\author{Ravikiran Bhat\\
Rubanraj Ravichandran\\
Ramesh Kumar}

\begin{document}
\maketitle
\newpage
\section{Exercise 2.1}
The equation 2.3 gives the weight adjustment $\Delta w_{kj}$ applied to synaptic weight $w_{kj}(n)$ according to the delata rule: \\
$$\Delta w_{kj} = \eta e_{k}(n) x_{j}(n)$$  
The equation 2.9 gives the weight adjustment $\Delta w_{kj}$ as: $$\Delta w_{kj} = \eta y_{k}(n) x_{j}(n)$$  
The main distinguishing feature between these two rules is that in case of the delta rule, the adjustment to synaptic weight of a neuron is proportional to the product of the error signal and input signal of the synapse, whereas the Hebb's rule gives the synaptic weight $w_{kj}$ for neuron k as a function of presynaptic signal (i.e, the input signal) and postsynaptic signal (i.e, the output signal).   
\section{Exercise 2.10:}

\end{document}